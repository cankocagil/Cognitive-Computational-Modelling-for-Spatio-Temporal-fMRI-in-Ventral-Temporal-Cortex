{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                        - Computational Neuroscience 2021-2022 Final Project -        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Project Name: Combinatorial Codes in Ventral Temporal Lobe for Visual Object Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap in d:\\python\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: pipreqs in d:\\python\\lib\\site-packages (0.4.10)\n",
      "Requirement already satisfied: yarg in d:\\python\\lib\\site-packages (from pipreqs) (0.1.9)\n",
      "Requirement already satisfied: docopt in d:\\python\\lib\\site-packages (from pipreqs) (0.6.2)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from yarg->pipreqs) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (2.10)\n",
      "Requirement already satisfied: lazypredict in d:\\python\\lib\\site-packages (0.2.9)\n",
      "Requirement already satisfied: joblib==1.0.0 in d:\\python\\lib\\site-packages (from lazypredict) (1.0.0)\n",
      "Requirement already satisfied: lightgbm==2.3.1 in d:\\python\\lib\\site-packages (from lazypredict) (2.3.1)\n",
      "Requirement already satisfied: numpy==1.19.1 in d:\\python\\lib\\site-packages (from lazypredict) (1.19.1)\n",
      "Requirement already satisfied: PyYAML==5.3.1 in d:\\python\\lib\\site-packages (from lazypredict) (5.3.1)\n",
      "Requirement already satisfied: six==1.15.0 in d:\\python\\lib\\site-packages (from lazypredict) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in d:\\python\\lib\\site-packages (from lazypredict) (0.23.1)\n",
      "Requirement already satisfied: xgboost==1.1.1 in d:\\python\\lib\\site-packages (from lazypredict) (1.1.1)\n",
      "Requirement already satisfied: scipy==1.5.4 in d:\\python\\lib\\site-packages (from lazypredict) (1.5.4)\n",
      "Requirement already satisfied: click==7.1.2 in d:\\python\\lib\\site-packages (from lazypredict) (7.1.2)\n",
      "Requirement already satisfied: pandas==1.0.5 in d:\\python\\lib\\site-packages (from lazypredict) (1.0.5)\n",
      "Requirement already satisfied: pytest==5.4.3 in d:\\python\\lib\\site-packages (from lazypredict) (5.4.3)\n",
      "Requirement already satisfied: tqdm==4.56.0 in d:\\python\\lib\\site-packages (from lazypredict) (4.56.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn==0.23.1->lazypredict) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\python\\lib\\site-packages (from pandas==1.0.5->lazypredict) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\python\\lib\\site-packages (from pandas==1.0.5->lazypredict) (2.8.1)\n",
      "Requirement already satisfied: packaging in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (20.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (20.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (1.9.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0; sys_platform == \"win32\" in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (1.4.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.13.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (8.6.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.4.4)\n",
      "Requirement already satisfied: wcwidth in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging->pytest==5.4.3->lazypredict) (2.4.7)\n",
      "Requirement already satisfied: nibabel in d:\\python\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\python\\lib\\site-packages (from nibabel) (20.4)\n",
      "Requirement already satisfied: numpy>=1.14 in d:\\python\\lib\\site-packages (from nibabel) (1.19.1)\n",
      "Requirement already satisfied: six in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel) (2.4.7)\n",
      "Requirement already satisfied: nilearn in d:\\python\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: joblib>=0.12 in d:\\python\\lib\\site-packages (from nilearn) (1.0.0)\n",
      "Requirement already satisfied: nibabel>=2.0.2 in d:\\python\\lib\\site-packages (from nilearn) (3.2.1)\n",
      "Requirement already satisfied: scipy>=0.19 in d:\\python\\lib\\site-packages (from nilearn) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in d:\\python\\lib\\site-packages (from nilearn) (0.23.1)\n",
      "Requirement already satisfied: pandas>=0.18.0 in d:\\python\\lib\\site-packages (from nilearn) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.11 in d:\\python\\lib\\site-packages (from nilearn) (1.19.1)\n",
      "Requirement already satisfied: requests>=2 in d:\\python\\lib\\site-packages (from nilearn) (2.24.0)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\python\\lib\\site-packages (from nibabel>=2.0.2->nilearn) (20.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn>=0.19->nilearn) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\python\\lib\\site-packages (from pandas>=0.18.0->nilearn) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\python\\lib\\site-packages (from pandas>=0.18.0->nilearn) (2.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (3.0.4)\n",
      "Requirement already satisfied: six in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (2.4.7)\n",
      "Requirement already up-to-date: kaleido in d:\\python\\lib\\site-packages (0.2.1)\n",
      "Scikit-learn is available, version 0.23.1\n",
      "Open-CV is available, version 4.5.1\n",
      "Seaborn is available, version 0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install umap\n",
    "!pip install pipreqs\n",
    "!pip install lazypredict\n",
    "!pip install nibabel\n",
    "!pip install nilearn\n",
    "!pip install -U kaleido\n",
    "\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print('Scikit-learn is available, version', sklearn.__version__)\n",
    "    \n",
    "except:\n",
    "    !pip install scikit-learn\n",
    "    \n",
    " \n",
    "try:\n",
    "    import cv2\n",
    "    print('Open-CV is available, version', cv2.__version__)\n",
    "    \n",
    "except:\n",
    "     !pip install opencv-python\n",
    "    \n",
    "   \n",
    "try:\n",
    "    import seaborn\n",
    "    print('Seaborn is available, version', seaborn.__version__)\n",
    "    \n",
    "except:\n",
    "     !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Version:  1.19.1\n",
      "Working Directory: \n",
      "  C:\\Users\\Administrator\\Desktop\\VOR\\sub notebooks\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# Basics:\n",
    "import numpy as np,pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import os, random, time, sys, copy, math, pickle\n",
    "\n",
    "# interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For plotting\n",
    "import plotly.io as plt_io\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "# Dimension Reduction Algorithms:\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import NMF\n",
    "import umap\n",
    "\n",
    "# Transformations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics:\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train-Test Splitter:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Classical ML algorithms:\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Utilies:\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For distance measurements:\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Extras:\n",
    "from abc import abstractmethod\n",
    "from typing import Callable, Iterable, List, Tuple\n",
    "\n",
    "# Set true for Google Colab:\n",
    "COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "    # To access Google Drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "\n",
    "    \n",
    "# For neuroimaging:\n",
    "from nibabel.testing import data_path\n",
    "from nilearn import plotting as nplt\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.image import index_img\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "\n",
    "\n",
    "\n",
    "print(\"NumPy Version: \", np.__version__)\n",
    "\n",
    "\n",
    "\n",
    "root_dir = r'C:\\Users\\Administrator\\Desktop\\VOR'\n",
    "os.chdir(root_dir)\n",
    "image_results_dir = os.path.join(root_dir, 'images')\n",
    "results_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "print('Working Directory: \\n ', root_dir)\n",
    "\n",
    "\n",
    "# Creating requirements.txt file\n",
    "!pip3 freeze > requirements.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 6 number of subjects in the experiment: \n",
    "haxby_dataset = datasets.fetch_haxby(subjects= [1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anat': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\anat.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\anat.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\anat.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\anat.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\anat.nii.gz',\n",
       "  None],\n",
       " 'func': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\bold.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\bold.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\bold.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\bold.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\bold.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj6\\\\bold.nii.gz'],\n",
       " 'session_target': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\labels.txt',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\labels.txt',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\labels.txt',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\labels.txt',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\labels.txt',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj6\\\\labels.txt'],\n",
       " 'mask_vt': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\mask4_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\mask4_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\mask4_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\mask4_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\mask4_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj6\\\\mask4_vt.nii.gz'],\n",
       " 'mask_face': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\mask8b_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\mask8b_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\mask8b_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\mask8b_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\mask8b_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj6\\\\mask8b_face_vt.nii.gz'],\n",
       " 'mask_house': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\mask8b_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\mask8b_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\mask8b_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\mask8b_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\mask8b_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj6\\\\mask8b_house_vt.nii.gz'],\n",
       " 'mask_face_little': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\mask8_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\mask8_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\mask8_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\mask8_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\mask8_face_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj6\\\\mask8_face_vt.nii.gz'],\n",
       " 'mask_house_little': ['C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj1\\\\mask8_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj2\\\\mask8_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj3\\\\mask8_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj4\\\\mask8_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj5\\\\mask8_house_vt.nii.gz',\n",
       "  'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\subj6\\\\mask8_house_vt.nii.gz'],\n",
       " 'mask': 'C:\\\\Users\\\\Administrator/nilearn_data\\\\haxby2001\\\\mask.nii.gz',\n",
       " 'description': b'Haxby 2001 results\\n\\n\\nNotes\\n-----\\nResults from a classical fMRI study that investigated the differences between\\nthe neural correlates of face versus object processing in the ventral visual\\nstream. Face and object stimuli showed widely distributed and overlapping\\nresponse patterns.\\n\\nContent\\n-------\\nThe \"simple\" dataset includes\\n    :\\'func\\': Nifti images with bold data\\n    :\\'session_target\\': Text file containing session data\\n    :\\'mask\\': Nifti images with employed mask\\n    :\\'session\\': Text file with condition labels\\n\\n\\nThe full dataset additionally includes\\n    :\\'anat\\': Nifti images with anatomical image\\n    :\\'func\\': Nifti images with bold data\\n    :\\'mask_vt\\': Nifti images with mask for ventral visual/temporal cortex\\n    :\\'mask_face\\': Nifti images with face-reponsive brain regions\\n    :\\'mask_house\\': Nifti images with house-reponsive brain regions\\n    :\\'mask_face_little\\': Spatially more constrained version of the above\\n    :\\'mask_house_little\\': Spatially more constrained version of the above\\n\\n\\nReferences\\n----------\\nFor more information see:\\nPyMVPA provides a tutorial using this dataset :\\nhttp://www.pymvpa.org/tutorial.html\\n\\nMore informations about its structure :\\nhttp://dev.pymvpa.org/datadb/haxby2001.html\\n\\n\\n`Haxby, J., Gobbini, M., Furey, M., Ishai, A., Schouten, J.,\\nand Pietrini, P. (2001). Distributed and overlapping representations of\\nfaces and objects in ventral temporal cortex. Science 293, 2425-2430.`\\n\\n\\nLicence: unknown.\\n'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haxby_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels  chunks\n",
       "0   rest       0\n",
       "1   rest       0\n",
       "2   rest       0\n",
       "3   rest       0\n",
       "4   rest       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load behavioral information\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\n",
    "behavioral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle\n",
      "cat\n",
      "chair\n",
      "face\n",
      "house\n",
      "rest\n",
      "scissors\n",
      "scrambledpix\n",
      "shoe\n"
     ]
    }
   ],
   "source": [
    "# Visual Stimuli Categories:\n",
    "for stimuli in np.unique(behavioral['labels']).tolist():\n",
    "    print(stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_categories = [\n",
    "                        'scissors',\n",
    "                        'face', \n",
    "                        'cat',\n",
    "                        'scrambledpix',\n",
    "                        'bottle',\n",
    "                        'chair',\n",
    "                        'shoe',\n",
    "                        'house'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First subject functional nifti images (4D) are at: C:\\Users\\Administrator/nilearn_data\\haxby2001\\subj1\\bold.nii.gz\n",
      "First subject functional nifti images (4D) are at: C:\\Users\\Administrator/nilearn_data\\haxby2001\\subj2\\bold.nii.gz\n",
      "First subject functional nifti images (4D) are at: C:\\Users\\Administrator/nilearn_data\\haxby2001\\subj3\\bold.nii.gz\n",
      "First subject functional nifti images (4D) are at: C:\\Users\\Administrator/nilearn_data\\haxby2001\\subj4\\bold.nii.gz\n",
      "First subject functional nifti images (4D) are at: C:\\Users\\Administrator/nilearn_data\\haxby2001\\subj5\\bold.nii.gz\n",
      "First subject functional nifti images (4D) are at: C:\\Users\\Administrator/nilearn_data\\haxby2001\\subj6\\bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "num_subjects = 6\n",
    "\n",
    "for subject in range(num_subjects):   \n",
    "\n",
    "    # 'func' is a list of filenames: one for each subject\n",
    "    fmri_filename = haxby_dataset.func[subject]\n",
    "\n",
    "    # print basic information on the dataset\n",
    "    print('First subject functional nifti images (4D) are at: %s' %\n",
    "          fmri_filename)  # 4D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864,)\n"
     ]
    }
   ],
   "source": [
    "# Creating conditional categories:\n",
    "conditions = behavioral['labels']\n",
    "\n",
    "# We ignore rest condition:\n",
    "condition_mask = conditions.isin(stimuli_categories).tolist()\n",
    "\n",
    "\n",
    "fmri_niimgs = index_img(fmri_filename, condition_mask)\n",
    "\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "# Convert to numpy array\n",
    "conditions = conditions.values\n",
    "print(conditions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 64, 64, 864)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (temporal dimension, spatial dimension 1, spatial dimension 2, # of experiments)\n",
    "fmri_niimgs.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864,)\n",
      "(864,)\n",
      "(864,)\n",
      "(864,)\n",
      "(864,)\n",
      "(864,)\n"
     ]
    }
   ],
   "source": [
    "for subject_id in range(num_subjects):\n",
    "    label = pd.read_csv(haxby_dataset.session_target[subject_id], delimiter=' ')\n",
    "    \n",
    "    # Creating conditional categories:\n",
    "    conditions = behavioral['labels']\n",
    "\n",
    "    condition_mask = conditions.isin(stimuli_categories).tolist()\n",
    "    conditions = conditions[condition_mask]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    conditions = conditions.values\n",
    "    print(conditions.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating stimuli to category and category to stimuli:\n",
    "stimuli2category = {\n",
    "                        'scissors'     : 0,\n",
    "                        'face'         : 1, \n",
    "                        'cat'          : 2,\n",
    "                        'scrambledpix' : 3,\n",
    "                        'bottle'       : 4,\n",
    "                        'chair'        : 5,\n",
    "                        'shoe'         : 6,\n",
    "                        'house'        : 7\n",
    "}\n",
    "\n",
    "category2stimuli = {category:stimuli for stimuli, category in stimuli2category.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_haxby_per_subject(subject_id:int = None,standardize:bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    \n",
    "        Given the subject id, fetch the haxby data in matrix format.\n",
    "        \n",
    "        Arguments:\n",
    "            - subject_id  (int) : Subject number from [1,6]\n",
    "            - standardize (bool): If true, masks are standardized\n",
    "            \n",
    "        Returns:\n",
    "            - data (Tuple[np.ndarray, np.ndarray, np.ndarray]) = Original 4-D data, Flattened + Masked Data, Label  \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # Getting the data file name:\n",
    "    spatio_temporal_data_path = haxby_dataset.func[subject_id]  \n",
    "   \n",
    "    # Getting labels:\n",
    "    behavioral = pd.read_csv(haxby_dataset.session_target[subject_id], delimiter = ' ')\n",
    "    \n",
    "    # Creating conditional categories:\n",
    "    conditions = behavioral['labels']\n",
    "    \n",
    "    # Creating masks for stimuli categories, (ignores rest conditions)\n",
    "    condition_mask = conditions.isin([*stimuli2category]).tolist()\n",
    "    \n",
    "    # Appylying masks to labels (categorical):\n",
    "    conditions = conditions[condition_mask]\n",
    "    \n",
    "    # Creating labels series (numerical):\n",
    "    categories = np.array([stimuli2category[stimulus] for stimulus in conditions])\n",
    "    \n",
    "    # Masking fMRI images: (shape = (40, 64, 64, 864))\n",
    "    fmri_niimgs = index_img(spatio_temporal_data_path, condition_mask)\n",
    "    \n",
    "    # Converting NumPy and transposing to (864, 40, 64, 64):\n",
    "    numpy_fmri = fmri_niimgs.get_data().transpose(3,0,1,2)\n",
    "    \n",
    "    masker = NiftiMasker(mask_img=haxby_dataset.mask_vt[subject_id],\n",
    "                         smoothing_fwhm=4,\n",
    "                         standardize=standardize,\n",
    "                         memory='nilearn_cache',\n",
    "                         memory_level=1)\n",
    "\n",
    "    masked = masker.fit_transform(fmri_niimgs)\n",
    "    \n",
    "    \n",
    "    return numpy_fmri,  masked, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [fetch_haxby_per_subject(subject_id) for subject_id in range(num_subjects)]\n",
    "fmri_imgs_mat, masks, categories = list(zip(*data))\n",
    "\n",
    "# Saving the data for future use:\n",
    "save(fmri_imgs_mat, 'fMRI_data')\n",
    "save(masks, 'masked_data')\n",
    "save(categories, 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading:\n",
    "fmri_imgs_mat, masks, categories = load('fMRI_data'), load('masked_data'), load('labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
