{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                        - Computational Neuroscience 2021-2022 Final Project -        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Project Name: Combinatorial Codes in Ventral Temporal Lobe for Visual Object Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap in d:\\python\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: pipreqs in d:\\python\\lib\\site-packages (0.4.10)\n",
      "Requirement already satisfied: yarg in d:\\python\\lib\\site-packages (from pipreqs) (0.1.9)\n",
      "Requirement already satisfied: docopt in d:\\python\\lib\\site-packages (from pipreqs) (0.6.2)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from yarg->pipreqs) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (1.25.11)\n",
      "Requirement already satisfied: lazypredict in d:\\python\\lib\\site-packages (0.2.9)\n",
      "Requirement already satisfied: numpy==1.19.1 in d:\\python\\lib\\site-packages (from lazypredict) (1.19.1)\n",
      "Requirement already satisfied: PyYAML==5.3.1 in d:\\python\\lib\\site-packages (from lazypredict) (5.3.1)\n",
      "Requirement already satisfied: joblib==1.0.0 in d:\\python\\lib\\site-packages (from lazypredict) (1.0.0)\n",
      "Requirement already satisfied: click==7.1.2 in d:\\python\\lib\\site-packages (from lazypredict) (7.1.2)\n",
      "Requirement already satisfied: pandas==1.0.5 in d:\\python\\lib\\site-packages (from lazypredict) (1.0.5)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in d:\\python\\lib\\site-packages (from lazypredict) (0.23.1)\n",
      "Requirement already satisfied: lightgbm==2.3.1 in d:\\python\\lib\\site-packages (from lazypredict) (2.3.1)\n",
      "Requirement already satisfied: tqdm==4.56.0 in d:\\python\\lib\\site-packages (from lazypredict) (4.56.0)\n",
      "Requirement already satisfied: xgboost==1.1.1 in d:\\python\\lib\\site-packages (from lazypredict) (1.1.1)\n",
      "Requirement already satisfied: six==1.15.0 in d:\\python\\lib\\site-packages (from lazypredict) (1.15.0)\n",
      "Requirement already satisfied: pytest==5.4.3 in d:\\python\\lib\\site-packages (from lazypredict) (5.4.3)\n",
      "Requirement already satisfied: scipy==1.5.4 in d:\\python\\lib\\site-packages (from lazypredict) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\python\\lib\\site-packages (from pandas==1.0.5->lazypredict) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\python\\lib\\site-packages (from pandas==1.0.5->lazypredict) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn==0.23.1->lazypredict) (2.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (8.6.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (20.3.0)\n",
      "Requirement already satisfied: packaging in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (20.4)\n",
      "Requirement already satisfied: py>=1.5.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (1.9.0)\n",
      "Requirement already satisfied: wcwidth in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.2.5)\n",
      "Requirement already satisfied: atomicwrites>=1.0; sys_platform == \"win32\" in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (1.4.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.13.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging->pytest==5.4.3->lazypredict) (2.4.7)\n",
      "Requirement already satisfied: nibabel in d:\\python\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\python\\lib\\site-packages (from nibabel) (20.4)\n",
      "Requirement already satisfied: numpy>=1.14 in d:\\python\\lib\\site-packages (from nibabel) (1.19.1)\n",
      "Requirement already satisfied: six in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel) (2.4.7)\n",
      "Requirement already satisfied: nilearn in d:\\python\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: nibabel>=2.0.2 in d:\\python\\lib\\site-packages (from nilearn) (3.2.1)\n",
      "Requirement already satisfied: requests>=2 in d:\\python\\lib\\site-packages (from nilearn) (2.24.0)\n",
      "Requirement already satisfied: joblib>=0.12 in d:\\python\\lib\\site-packages (from nilearn) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.11 in d:\\python\\lib\\site-packages (from nilearn) (1.19.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in d:\\python\\lib\\site-packages (from nilearn) (0.23.1)\n",
      "Requirement already satisfied: pandas>=0.18.0 in d:\\python\\lib\\site-packages (from nilearn) (1.0.5)\n",
      "Requirement already satisfied: scipy>=0.19 in d:\\python\\lib\\site-packages (from nilearn) (1.5.4)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\python\\lib\\site-packages (from nibabel>=2.0.2->nilearn) (20.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (2020.6.20)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn>=0.19->nilearn) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\python\\lib\\site-packages (from pandas>=0.18.0->nilearn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\python\\lib\\site-packages (from pandas>=0.18.0->nilearn) (2020.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (2.4.7)\n",
      "Requirement already satisfied: six in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (1.15.0)\n",
      "Requirement already up-to-date: kaleido in d:\\python\\lib\\site-packages (0.2.1)\n",
      "Scikit-learn is available, version 0.23.1\n",
      "Open-CV is available, version 4.5.1\n",
      "Seaborn is available, version 0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install umap\n",
    "!pip install pipreqs\n",
    "!pip install lazypredict\n",
    "!pip install nibabel\n",
    "!pip install nilearn\n",
    "!pip install -U kaleido\n",
    "\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print('Scikit-learn is available, version', sklearn.__version__)\n",
    "    \n",
    "except:\n",
    "    !pip install scikit-learn\n",
    "    \n",
    " \n",
    "try:\n",
    "    import cv2\n",
    "    print('Open-CV is available, version', cv2.__version__)\n",
    "    \n",
    "except:\n",
    "     !pip install opencv-python\n",
    "    \n",
    "   \n",
    "try:\n",
    "    import seaborn\n",
    "    print('Seaborn is available, version', seaborn.__version__)\n",
    "    \n",
    "except:\n",
    "     !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Version:  1.19.1\n",
      "Working Directory: \n",
      "  C:\\Users\\Administrator\\Desktop\\VOR\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# Basics:\n",
    "import numpy as np,pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import os, random, time, sys, copy, math, pickle\n",
    "\n",
    "# interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For plotting\n",
    "import plotly.io as plt_io\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "# Dimension Reduction Algorithms:\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import NMF\n",
    "import umap\n",
    "\n",
    "# Transformations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics:\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train-Test Splitter:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Classical ML algorithms:\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Utilies:\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For distance measurements:\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Extras:\n",
    "from abc import abstractmethod\n",
    "from typing import Callable, Iterable, List, Tuple\n",
    "\n",
    "# Set true for Google Colab:\n",
    "COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "    # To access Google Drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "\n",
    "    \n",
    "# For neuroimaging:\n",
    "from nibabel.testing import data_path\n",
    "from nilearn import plotting as nplt\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.image import index_img\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "\n",
    "\n",
    "\n",
    "print(\"NumPy Version: \", np.__version__)\n",
    "\n",
    "\n",
    "root_dir = r'C:\\Users\\Administrator\\Desktop\\VOR'\n",
    "os.chdir(root_dir)\n",
    "image_results_dir = os.path.join(root_dir, 'images')\n",
    "results_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "print('Working Directory: \\n ', root_dir)\n",
    "\n",
    "\n",
    "# Creating requirements.txt file\n",
    "!pip3 freeze > requirements.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.timers import timeit\n",
    "from utils.metrics import accuracy, confusion_matrix, visualize_confusion_matrix\n",
    "from utils.savers import save, save_obj, load, load_obj\n",
    "from utils.reproduce import random_seed\n",
    "from dataset.fetch_data_matrix import fetch_from_haxby\n",
    "from visualizer.plot2D import plot_2d\n",
    "from visualizer.plot3D import plot_3d  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   One Shot ML Classifiers\n",
    "\n",
    "Applied Algorithms:\n",
    "\n",
    "    * LinearSVC\n",
    "    * SGDClassifier\n",
    "    * MLPClassifier\n",
    "    * Perceptron\n",
    "    * LogisticRegression\n",
    "    * LogisticRegressionCV\n",
    "    * SVC\n",
    "    * CalibratedClassifierCV\n",
    "    * PassiveAggressiveClassifier\n",
    "    * LabelPropagation\n",
    "    * LabelSpreading\n",
    "    * RandomForestClassifier\n",
    "    * GradientBoostingClassifier\n",
    "    * QuadraticDiscriminantAnalysis\n",
    "    * RidgeClassifierCV\n",
    "    * RidgeClassifier\n",
    "    * AdaBoostClassifier\n",
    "    * ExtraTreesClassifier\n",
    "    * KNeighborsClassifier\n",
    "    * BaggingClassifier\n",
    "    * BernoulliNB\n",
    "    * LinearDiscriminantAnalysis\n",
    "    * GaussianNB\n",
    "    * NuSVC\n",
    "    * DecisionTreeClassifier\n",
    "    * NearestCentroid\n",
    "    * ExtraTreeClassifier\n",
    "    * CheckingClassifier\n",
    "    * DummyClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Loading:\n",
    "fmri_imgs_mat, masks, categories = load('fMRI_data'), load('masked_data'), load('labels')\n",
    "\n",
    "\n",
    "predictions_per_subject = list()\n",
    "\n",
    "\n",
    "for subject_id, (mask, category) in enumerate(zip(masks, categories)):\n",
    "    \n",
    "    print(f'Subject id: {subject_id}')\n",
    "  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(mask, category, test_size=0.3, random_state=42)\n",
    "    \n",
    "    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "    models, predictions = clf.fit(X_train, X_test, y_train, y_test)    \n",
    "    \n",
    "    models.to_csv(os.path.join(results_dir, f'Subject_{subject_id}_lazy_results.csv'))\n",
    "\n",
    "    print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FREM : Ensembling of Regularized Models for Robust Decoding (SVC - L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FREM uses an implicit spatial regularization through fast clustering and aggregates a high number of estimators trained on various splits of the training set, thus returning a very robust decoder at a lower computational cost than other spatially regularized methods\n",
    "\n",
    "---\n",
    "\n",
    "FREM ensembling procedure yields an important improvement of decoding accuracy on this simple example compared to fitting only one model per fold and the clustering mechanism keeps its computational cost reasonable even on heavier examples. Here we ensembled several instances of l2-SVC, but FREMClassifier also works with ridge or logistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import FREMClassifier\n",
    "from nilearn.image import index_img\n",
    "    \n",
    "models_path = os.path.join(root_dir, 'models')\n",
    "num_subjects = 6\n",
    "\n",
    "for subject_id in range(num_subjects):\n",
    "    \n",
    "    print(f'Subject id: {subject_id}')\n",
    "\n",
    "    behavioral = pd.read_csv(haxby_dataset.session_target[subject_id], sep=\" \")\n",
    "\n",
    "    conditions = behavioral['labels']\n",
    "    condition_mask = conditions.isin([*stimuli2category])\n",
    "\n",
    "    # Split data into train and test samples, using the chunks\n",
    "    condition_mask_train = (condition_mask) & (behavioral['chunks'] <= 8)\n",
    "    condition_mask_test = (condition_mask) & (behavioral['chunks'] > 8)\n",
    "   \n",
    "   \n",
    "    filenames = haxby_dataset.func[subject_id]\n",
    "    X_train = index_img(filenames, condition_mask_train)\n",
    "    X_test = index_img(filenames, condition_mask_test)\n",
    "    y_train = conditions[condition_mask_train].values\n",
    "    y_test = conditions[condition_mask_test].values    \n",
    "    \n",
    "    masker = NiftiMasker(mask_img=haxby_dataset.mask_vt[subject_id],\n",
    "                         smoothing_fwhm=4,\n",
    "                         standardize=True,\n",
    "                         memory='nilearn_cache',\n",
    "                         memory_level=1)\n",
    "\n",
    "    #masked = masker.fit_transform(fmri_niimgs)\n",
    "    \n",
    "    \n",
    "    decoder = FREMClassifier(estimator='svc', cv=10, mask = masker)\n",
    "\n",
    "    # Fit model on train data and predict on test data\n",
    "    decoder.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = decoder.predict(X_test)\n",
    "    \n",
    "    report = pd.DataFrame(classification_report(y_test, y_pred, output_dict = True)).T      \n",
    "    report.to_csv(os.path.join(results_dir, f'Subject_{subject_id}_FREM_results.csv')) \n",
    "    \n",
    "    scores = pd.DataFrame(decoder.cv_scores_).T\n",
    "    scores.to_csv(os.path.join(results_dir, f'Subject_{subject_id}_FREMCV_results.csv')) \n",
    "    \n",
    "    save_obj(decoder, os.path.join(models_path, f'Subject_{subject_id}_FREM_model'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FREM : Ensembling of Regularized Models for Robust Decoding (Logistic Regression - L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import FREMClassifier\n",
    "from nilearn.image import index_img\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "cv = LeaveOneGroupOut()  \n",
    "models_path = os.path.join(root_dir, 'models')\n",
    "num_subjects = 6\n",
    "\n",
    "for subject_id in range(num_subjects):\n",
    "    \n",
    "    print(f'Subject id: {subject_id}')\n",
    "\n",
    "    behavioral = pd.read_csv(haxby_dataset.session_target[subject_id], sep=\" \")\n",
    "\n",
    "    conditions = behavioral['labels']\n",
    "    condition_mask = conditions.isin([*stimuli2category]) \n",
    "    \n",
    "    filenames = haxby_dataset.func[subject_id]\n",
    "    X_train = index_img(filenames, condition_mask)  \n",
    "    y_train = conditions[condition_mask].values\n",
    "    \n",
    "    decoder = FREMClassifier(estimator='logistic_l2',\n",
    "                             cv=10,\n",
    "                             mask = NiftiMasker(mask_img=haxby_dataset.mask_vt[subject_id],\n",
    "                                                 smoothing_fwhm=4,\n",
    "                                                 standardize=True,\n",
    "                                                 memory='nilearn_cache',\n",
    "                                                 memory_level=1)\n",
    "                            )\n",
    "\n",
    "    # Fit model on train data and predict on test data:\n",
    "    decoder.fit(X_train, y_train)\n",
    "    \n",
    "    # Saving:\n",
    "    scores = pd.DataFrame(decoder.cv_scores_).T\n",
    "    scores.to_csv(os.path.join(results_dir, f'Subject_{subject_id}_FREMLogisticRegressionCV_results.csv'))     \n",
    "    save_obj(decoder, os.path.join(models_path, f'Subject_{subject_id}_FREMLogisticRegressionCV_model'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_results_dir = os.path.join(root_dir,'images/results')\n",
    "\n",
    "subject_id = 5\n",
    "decoder = load_obj(os.path.join(models_path, f'Subject_{subject_id}_FREM_model'))\n",
    "\n",
    "weight_img = decoder.coef_img_[\"face\"]\n",
    "filenames = haxby_dataset.func[subject_id]\n",
    "\n",
    "\n",
    "plotting.plot_stat_map(weight_img,\n",
    "                       bg_img = mean_img(filenames),\n",
    "                       title=f\"FREM: Accuracy Score for Face Stimuli: {np.mean(decoder.cv_scores_['face']).round(2)}\",\n",
    "                       cut_coords=(-52, -5),\n",
    "                       display_mode=\"yz\",\n",
    "                       output_file= os.path.join(image_results_dir, 'FREM_face.png'),\n",
    "                       )\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 5\n",
    "decoder = load_obj(os.path.join(models_path, f'Subject_{subject_id}_FREM_model'))\n",
    "\n",
    "weight_img = decoder.coef_img_[\"house\"]\n",
    "filenames = haxby_dataset.func[subject_id]\n",
    "\n",
    "plotting.plot_stat_map(weight_img,\n",
    "                       bg_img = mean_img(filenames),\n",
    "                       title=f\"FREM: Accuracy Score: {np.mean(decoder.cv_scores_['house']).round(2)}\",\n",
    "                       cut_coords=(-52, -5),\n",
    "                       #output_file= os.path.join(image_results_dir, 'FREM_house.png'),\n",
    "                       display_mode=\"yz\")\n",
    "\n",
    "\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 0\n",
    "decoder = load_obj(os.path.join(models_path, f'Subject_{subject_id}_FREM_model'))\n",
    "\n",
    "weight_img = decoder.coef_img_[\"face\"]\n",
    "\n",
    "\n",
    "plotting.plot_stat_map(weight_img,\n",
    "                       bg_img=haxby_dataset.anat[subject_id],\n",
    "                       title='FREM (SVC-L2) Discriminating weights',\n",
    "                       #output_file= os.path.join(image_results_dir, 'FREM (SVC-L2) Discriminating weights.png'),\n",
    "                       )\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 0\n",
    "decoder = load_obj(os.path.join(models_path, f'Subject_{subject_id}_FREM_model'))\n",
    "\n",
    "weight_img = decoder.coef_img_[\"face\"]\n",
    "\n",
    "\n",
    "plotting.plot_stat_map(weight_img,\n",
    "                       bg_img=haxby_dataset.anat[subject_id],\n",
    "                       title='FREM (SVC-L2) Discriminating weights',\n",
    "                       dim = -1,\n",
    "                       #output_file= os.path.join(image_results_dir, 'FREM (SVC-L2) Discriminating weights anat.png')\n",
    "                      )\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
