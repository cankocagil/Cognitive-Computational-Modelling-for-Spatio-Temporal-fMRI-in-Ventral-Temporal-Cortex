{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                        - Computational Neuroscience 2021-2022 Final Project -        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Project Name: Combinatorial Codes in Ventral Temporal Lobe for Visual Object Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap in d:\\python\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: pipreqs in d:\\python\\lib\\site-packages (0.4.10)\n",
      "Requirement already satisfied: yarg in d:\\python\\lib\\site-packages (from pipreqs) (0.1.9)\n",
      "Requirement already satisfied: docopt in d:\\python\\lib\\site-packages (from pipreqs) (0.6.2)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from yarg->pipreqs) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\python\\lib\\site-packages (from requests->yarg->pipreqs) (2.10)\n",
      "Requirement already satisfied: lazypredict in d:\\python\\lib\\site-packages (0.2.9)\n",
      "Requirement already satisfied: pandas==1.0.5 in d:\\python\\lib\\site-packages (from lazypredict) (1.0.5)\n",
      "Requirement already satisfied: pytest==5.4.3 in d:\\python\\lib\\site-packages (from lazypredict) (5.4.3)\n",
      "Requirement already satisfied: numpy==1.19.1 in d:\\python\\lib\\site-packages (from lazypredict) (1.19.1)\n",
      "Requirement already satisfied: six==1.15.0 in d:\\python\\lib\\site-packages (from lazypredict) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in d:\\python\\lib\\site-packages (from lazypredict) (0.23.1)\n",
      "Requirement already satisfied: scipy==1.5.4 in d:\\python\\lib\\site-packages (from lazypredict) (1.5.4)\n",
      "Requirement already satisfied: xgboost==1.1.1 in d:\\python\\lib\\site-packages (from lazypredict) (1.1.1)\n",
      "Requirement already satisfied: click==7.1.2 in d:\\python\\lib\\site-packages (from lazypredict) (7.1.2)\n",
      "Requirement already satisfied: joblib==1.0.0 in d:\\python\\lib\\site-packages (from lazypredict) (1.0.0)\n",
      "Requirement already satisfied: PyYAML==5.3.1 in d:\\python\\lib\\site-packages (from lazypredict) (5.3.1)\n",
      "Requirement already satisfied: lightgbm==2.3.1 in d:\\python\\lib\\site-packages (from lazypredict) (2.3.1)\n",
      "Requirement already satisfied: tqdm==4.56.0 in d:\\python\\lib\\site-packages (from lazypredict) (4.56.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\python\\lib\\site-packages (from pandas==1.0.5->lazypredict) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\python\\lib\\site-packages (from pandas==1.0.5->lazypredict) (2.8.1)\n",
      "Requirement already satisfied: py>=1.5.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (1.9.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (20.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (8.6.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.4.4)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (0.2.5)\n",
      "Requirement already satisfied: packaging in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (20.4)\n",
      "Requirement already satisfied: atomicwrites>=1.0; sys_platform == \"win32\" in d:\\python\\lib\\site-packages (from pytest==5.4.3->lazypredict) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn==0.23.1->lazypredict) (2.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging->pytest==5.4.3->lazypredict) (2.4.7)\n",
      "Requirement already satisfied: nibabel in d:\\python\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\python\\lib\\site-packages (from nibabel) (20.4)\n",
      "Requirement already satisfied: numpy>=1.14 in d:\\python\\lib\\site-packages (from nibabel) (1.19.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel) (2.4.7)\n",
      "Requirement already satisfied: six in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel) (1.15.0)\n",
      "Requirement already satisfied: nilearn in d:\\python\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: joblib>=0.12 in d:\\python\\lib\\site-packages (from nilearn) (1.0.0)\n",
      "Requirement already satisfied: nibabel>=2.0.2 in d:\\python\\lib\\site-packages (from nilearn) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.11 in d:\\python\\lib\\site-packages (from nilearn) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.19 in d:\\python\\lib\\site-packages (from nilearn) (1.5.4)\n",
      "Requirement already satisfied: requests>=2 in d:\\python\\lib\\site-packages (from nilearn) (2.24.0)\n",
      "Requirement already satisfied: pandas>=0.18.0 in d:\\python\\lib\\site-packages (from nilearn) (1.0.5)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in d:\\python\\lib\\site-packages (from nilearn) (0.23.1)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\python\\lib\\site-packages (from nibabel>=2.0.2->nilearn) (20.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\python\\lib\\site-packages (from requests>=2->nilearn) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\python\\lib\\site-packages (from pandas>=0.18.0->nilearn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\python\\lib\\site-packages (from pandas>=0.18.0->nilearn) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn>=0.19->nilearn) (2.1.0)\n",
      "Requirement already satisfied: six in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\python\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (2.4.7)\n",
      "Requirement already up-to-date: kaleido in d:\\python\\lib\\site-packages (0.2.1)\n",
      "Scikit-learn is available, version 0.23.1\n",
      "Open-CV is available, version 4.5.1\n",
      "Seaborn is available, version 0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install umap\n",
    "!pip install pipreqs\n",
    "!pip install lazypredict\n",
    "!pip install nibabel\n",
    "!pip install nilearn\n",
    "!pip install -U kaleido\n",
    "\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print('Scikit-learn is available, version', sklearn.__version__)\n",
    "    \n",
    "except:\n",
    "    !pip install scikit-learn\n",
    "    \n",
    " \n",
    "try:\n",
    "    import cv2\n",
    "    print('Open-CV is available, version', cv2.__version__)\n",
    "    \n",
    "except:\n",
    "     !pip install opencv-python\n",
    "    \n",
    "   \n",
    "try:\n",
    "    import seaborn\n",
    "    print('Seaborn is available, version', seaborn.__version__)\n",
    "    \n",
    "except:\n",
    "     !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Version:  1.19.1\n",
      "Working Directory: \n",
      "  C:\\Users\\Administrator\\Desktop\\VOR\\sub notebooks\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# Basics:\n",
    "import numpy as np,pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import os, random, time, sys, copy, math, pickle\n",
    "\n",
    "# interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For plotting\n",
    "import plotly.io as plt_io\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "# Dimension Reduction Algorithms:\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import NMF\n",
    "import umap\n",
    "\n",
    "# Transformations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics:\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train-Test Splitter:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Classical ML algorithms:\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Utilies:\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For distance measurements:\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Extras:\n",
    "from abc import abstractmethod\n",
    "from typing import Callable, Iterable, List, Tuple\n",
    "\n",
    "# Set true for Google Colab:\n",
    "COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "    # To access Google Drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "\n",
    "    \n",
    "# For neuroimaging:\n",
    "from nibabel.testing import data_path\n",
    "from nilearn import plotting as nplt\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.image import index_img\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "\n",
    "\n",
    "\n",
    "print(\"NumPy Version: \", np.__version__)\n",
    "\n",
    "\n",
    "root_dir = r'C:\\Users\\Administrator\\Desktop\\VOR'\n",
    "os.chdir(root_dir)\n",
    "image_results_dir = os.path.join(root_dir, 'images')\n",
    "results_dir = os.path.join(root_dir, 'results')\n",
    "\n",
    "print('Working Directory: \\n ', root_dir)\n",
    "\n",
    "\n",
    "# Creating requirements.txt file\n",
    "!pip3 freeze > requirements.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# PyTorch's versions:\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "# We will be working with GPU:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device : ' , device)\n",
    "\n",
    "# Number of GPUs available. \n",
    "num_GPU = torch.cuda.device_count()\n",
    "print('Number of GPU : ', num_GPU)\n",
    "\n",
    "\n",
    "# Creating stimuli to category and category to stimuli:\n",
    "stimuli2category = {\n",
    "                        'scissors'     : 0,\n",
    "                        'face'         : 1, \n",
    "                        'cat'          : 2,\n",
    "                        'scrambledpix' : 3,\n",
    "                        'bottle'       : 4,\n",
    "                        'chair'        : 5,\n",
    "                        'shoe'         : 6,\n",
    "                        'house'        : 7\n",
    "}\n",
    "\n",
    "category2stimuli = {category:stimuli for stimuli, category in stimuli2category.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fMRIDataset(torch.utils.data.Dataset):\n",
    "    scaler = MinMaxScaler()\n",
    "    def __init__(self, \n",
    "                 mode:str = 'fMRI',\n",
    "                 transforms = None,\n",
    "                 fetch_from_path:bool = True,\n",
    "                 prepare_for_transformer:bool = False):\n",
    "        \n",
    "        assert mode in ['fMRI','mask'], 'Please provide fMRI or Mask type of mode!'\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.num_class = len(stimuli2category) or len(category2stimuli)\n",
    "\n",
    "        self.batch_data_path = 'batch_fMRI'\n",
    "        self.batch_label_path = 'batch_label'\n",
    "        self.batch_mask_path = 'batch_masks'\n",
    "        \n",
    "        if prepare_for_transformer:\n",
    "            self.batch_data_path = 'batch_fMRI_transformer'\n",
    "            self.batch_data_path = 'batch_label_transformer'\n",
    "\n",
    "        \n",
    "        batched_data_path = os.path.join(root_dir, self.batch_data_path)\n",
    "        bacthed_label_path = os.path.join(root_dir, self.batch_label_path)  \n",
    "        bacthed_mask_path = os.path.join(root_dir, self.batch_mask_path)  \n",
    "        \n",
    "        \n",
    "        if mode == 'fMRI':            \n",
    "            if fetch_from_path: \n",
    "                if os.path.exists(batched_data_path + '.npy') and os.path.exists(bacthed_label_path  + '.npy'):   \n",
    "                    \n",
    "                    print(f'Data is fetching from {root_dir}')\n",
    "                    self.data = load(batched_data_path)\n",
    "                    self.labels = load(bacthed_label_path)   \n",
    "                    \n",
    "                else:\n",
    "                    raise NoneError(\"Object not constructed. Cannot access a 'None' object.\")\n",
    "            else:\n",
    "                              \n",
    "                self.data = np.concatenate(load('fMRI_data'), axis = 0)\n",
    "                self.labels = np.concatenate(load('labels'), axis = 0)\n",
    "                \n",
    "                if prepare_for_transformer:\n",
    "                    self.prepare_transformer()\n",
    "                    \n",
    "                save(self.data, batched_data_path)\n",
    "                save(self.labels, bacthed_label_path)                                    \n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "                   \n",
    "\n",
    "        \n",
    "        assert self.labels.shape[0] == self.data.shape[0], ' # of Targets and Data samples does not match!'\n",
    "            \n",
    "    \n",
    "    def prepare_transformer(self):\n",
    "        self.data = self.data[:, 1:, :, :, ].reshape(-1, 64, 64, 3)                         \n",
    "        self.labels = np.repeat(self.labels, repeats = 13, axis = 0)\n",
    "-      \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = self.data[idx]\n",
    "               \n",
    "        \n",
    "        if image.shape == torch.Size([64, 3, 64]):\n",
    "            image = image.permute(1,0,2)\n",
    "                 \n",
    "        #assert image.shape == torch.Size([3, 64, 64]), 'Mismatch Image Dimension!'\n",
    "            \n",
    "        label = self.labels[idx].reshape(1,)\n",
    "        label = torch.as_tensor(label, dtype=torch.int, device=device)\n",
    "  \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "          \n",
    "        return image, label     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize():\n",
    "    def __call__(self, image):\n",
    "        max_val = image.max()\n",
    "        return image / max_val\n",
    "    \n",
    "class TorchTensor():\n",
    "    def __call__(self, image):\n",
    "        return torch.as_tensor(image, dtype=torch.float, device=device)\n",
    "        \n",
    "class MeanNormalize():\n",
    "    def __call__(self, image):\n",
    "        return F.normalize(image)\n",
    "    \n",
    "# [batch * channel(# of channels of each image) * depth(# of frames) * height * width]\n",
    "class Make3D():\n",
    "    def __call__(self, image):        \n",
    "        return image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                #transforms.ColorJitter([0.9,0.9]),\n",
    "                                #transforms.RandomGrayscale(p = 0.3),\n",
    "                                #transforms.RandomAffine((-30,30)),\n",
    "                                #transforms.RandomPerspective(),\n",
    "                                #transforms.GaussianBlur(3),\n",
    "                                #transforms.RandomHorizontalFlip(p = 0.2),\n",
    "                                #transforms.RandomVerticalFlip(p = 0.2),\n",
    "\n",
    "                                #Important parts, above can be ignored\n",
    "                                #transforms.Resize((224,224)),\n",
    "                                #transforms.CenterCrop(224),\n",
    "                                Normalize(),\n",
    "                                TorchTensor()    \n",
    "                                #transforms.ToTensor(),\n",
    "                                                                \n",
    "])\n",
    "\n",
    "\n",
    "fMRI_dataset = fMRIDataset(transforms = transform, fetch_from_path = True)\n",
    "\n",
    "\n",
    "break_point = len(fMRI_dataset) - 100 \n",
    "train_dataset = torch.utils.data.Subset(fMRI_dataset, indices = range(break_point))\n",
    "val_dataset = torch.utils.data.Subset(fMRI_dataset, indices = range(break_point, len(fMRI_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          shuffle = False,\n",
    "                                          batch_size = batch_size,\n",
    "                                          drop_last = True,\n",
    "                                          )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                          shuffle = False,\n",
    "                                          batch_size = batch_size,\n",
    "                                          drop_last = True,\n",
    "                                          )\n",
    "x, y = next(iter(train_loader))\n",
    "\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils import utils_torch\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, print_freq, apex=False):\n",
    "    model.train()\n",
    "    metric_logger = utils_torch.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils_torch.SmoothedValue(window_size=1, fmt='{value}'))\n",
    "    metric_logger.add_meter('img/s', utils_torch.SmoothedValue(window_size=10, fmt='{value}'))\n",
    "\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        start_time = time.time()\n",
    "        image, target = image.to(device), target.to(device).squeeze(-1).long()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc1, acc5 = utils_torch.accuracy(output, target, topk=(1, 5))\n",
    "        batch_size = image.shape[0]\n",
    "        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, device, print_freq=100):\n",
    "    model.eval()\n",
    "    metric_logger = utils_torch.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "    with torch.no_grad():\n",
    "        for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "            image = image.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True).squeeze(-1).long()\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            acc1, acc5 = utils_torch.accuracy(output, target, topk=(1,5))\n",
    "            # FIXME need to take into account that the datasets\n",
    "            # could have been padded in distributed setup\n",
    "            batch_size = image.shape[0]\n",
    "            metric_logger.update(loss=loss.item())\n",
    "            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "\n",
    "    print(' * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, top5=metric_logger.acc5))\n",
    "    return metric_logger.acc1.global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,model = None):\n",
    "        super(Model,self).__init__()\n",
    "        if model is not None:\n",
    "            self.model = model    \n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "            self.conv_block(40,  60,  0.1),\n",
    "            self.conv_block(60,  80,  0.15),\n",
    "            self.conv_block(80,  128, 0.25),\n",
    "            self.conv_block(128, 256, 0.3),\n",
    "            nn.Flatten(),\n",
    "            self.linear_block(1024, 256, 0.4),\n",
    "            self.linear_block(256, 128, 0.4),\n",
    "            nn.Linear(128, 8)\n",
    ")     \n",
    "\n",
    "    def forward(self,img):    \n",
    "        return self.model(img)  \n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(in_channel, out_channel, p):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout2d(p)\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_block(in_ftrs,out_ftrs,p):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_ftrs,out_ftrs),\n",
    "            nn.BatchNorm1d(num_features=out_ftrs),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p)\n",
    "            )\n",
    "\n",
    "net = Model().to(device)\n",
    "print('Traniable parameter of the model: ' , sum(param.numel() for param in net.parameters() if param.requires_grad == True))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "print_freq = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(net, criterion, optimizer, train_loader, device, epoch, print_freq, apex=False)\n",
    "    \n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(net, criterion, val_loader, device, print_freq)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3D(nn.Module):\n",
    "    def __init__(self, model = None):\n",
    "        super(Model,self).__init__()\n",
    "        if model is not None:\n",
    "            self.model = model    \n",
    "        else:\n",
    "            self.conv3d = nn.Sequential(               \n",
    "                            self.conv_block(1, 32, (3,3,3)),\n",
    "                            self.conv_block(32, 64, (3,3,3)),\n",
    "                            self.conv_block(64, 128, (3,3,3)),\n",
    "                            self.conv_block(128, 512, (3,3,3)),\n",
    "                            nn.Flatten()\n",
    "            )           \n",
    "            \n",
    "            self.linear = nn.Sequential(\n",
    "                            self.linear_block(1024,512,0.4),\n",
    "                            self.linear_block(512,256,0.4),\n",
    "                            nn.Linear(256, 8)\n",
    "            ) \n",
    "            \n",
    "            \n",
    "            self.model = nn.Sequential(                                    \n",
    "                           self.conv3d,\n",
    "                           self.linear            \n",
    "            )\n",
    "\n",
    "    def forward(self,img):    \n",
    "        return self.model(img)  \n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(in_channel, out_channel, p):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channel,  out_channel,  (3,3,3) , (3,3,3)),\n",
    "            nn.BatchNorm3d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(2,2),\n",
    "            nn.Dropout3d(p)\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_block(in_ftrs,out_ftrs,p):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_ftrs,out_ftrs),\n",
    "            nn.BatchNorm1d(num_features=out_ftrs),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p)\n",
    "            )\n",
    "\n",
    "net = Model().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma = 0.7)\n",
    "\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "print_freq = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(net, criterion, optimizer, train_loader, device, epoch, print_freq, apex=False)\n",
    "    \n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(net, criterion, val_loader, device, print_freq)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
